{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###   imdb_dense_torch_001.ipynb\n",
    "\n",
    "1. Run under a PyTorch virtual env\n",
    "\n",
    "2. Get the imdb_data from the files created in imdb_dense_torch_0000.ipynb\n",
    "\n",
    "3. Modify the data structures as necessary\n",
    "\n",
    "4. Use the data to test a PyTorch Dense model with parameters similar to the ones we used under TF/Keras\n",
    "\n",
    "5. Compare results vis-a vis the results using TF Dense Models \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mimic the IMDB Data, which is typically set as list of (review, label) tuples.\n",
    "# The reviews are lists of word indices, from 1...10000, going from more important to less important\n",
    "# The labels are 0 or 1, indicating a bad or good review, respectively.\n",
    "\n",
    "# I created the data manually, to ensure that the data preprocessing, the models etc, work well.\n",
    "# There are only three different lists in this data to ensure that models can learn and predict\n",
    "# with high accuracy. Results with low accuracy will reveal problems in the data pre-processing,\n",
    "# the models configurations, or both...\n",
    "\n",
    "imdb_fake_data = [\n",
    "    ([1, 2, 3, 4, 5], 1),  # Review: [word1, word2, ...], Label: 1\n",
    "    ([6, 7, 8], 0),        # Review: [word1, word2, ...], Label: 0\n",
    "    ([1, 2, 3, 4, 5], 1),\n",
    "    ([1, 2, 3, 4, 5], 1),\n",
    "    ([1, 2, 3, 4, 5], 1),\n",
    "    ([1, 2, 3, 4, 5], 1),\n",
    "    ([1, 2, 3, 4, 5], 1),\n",
    "    ([1, 2, 3, 4, 5], 1),\n",
    "    ([6, 7, 8], 0),\n",
    "    ([6, 7, 8], 0),\n",
    "    ([6, 7, 8], 0),\n",
    "    ([6, 7, 8], 0),\n",
    "    ([6, 7, 8], 0),\n",
    "    ([15, 16], 0),\n",
    "    ([15, 16], 0),\n",
    "    ([15, 16], 0),\n",
    "    ([15, 16], 0),\n",
    "    ([15, 16], 0),\n",
    "    ([15, 16], 0),\n",
    "    ([15, 16], 0),\n",
    "    ([15, 16], 0),\n",
    "    ([15, 16], 0),\n",
    "    # ... more data\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orig fake data\n",
      "[([1, 2, 3, 4, 5], 1), ([6, 7, 8], 0), ([1, 2, 3, 4, 5], 1), ([1, 2, 3, 4, 5], 1), ([1, 2, 3, 4, 5], 1), ([1, 2, 3, 4, 5], 1), ([1, 2, 3, 4, 5], 1), ([1, 2, 3, 4, 5], 1), ([6, 7, 8], 0), ([6, 7, 8], 0), ([6, 7, 8], 0), ([6, 7, 8], 0), ([6, 7, 8], 0), ([15, 16], 0), ([15, 16], 0), ([15, 16], 0), ([15, 16], 0), ([15, 16], 0), ([15, 16], 0), ([15, 16], 0), ([15, 16], 0), ([15, 16], 0)]\n",
      "fake data from disk\n",
      "[([1, 2, 3, 4, 5], 1), ([6, 7, 8], 0), ([1, 2, 3, 4, 5], 1), ([1, 2, 3, 4, 5], 1), ([1, 2, 3, 4, 5], 1), ([1, 2, 3, 4, 5], 1), ([1, 2, 3, 4, 5], 1), ([1, 2, 3, 4, 5], 1), ([6, 7, 8], 0), ([6, 7, 8], 0), ([6, 7, 8], 0), ([6, 7, 8], 0), ([6, 7, 8], 0), ([15, 16], 0), ([15, 16], 0), ([15, 16], 0), ([15, 16], 0), ([15, 16], 0), ([15, 16], 0), ([15, 16], 0), ([15, 16], 0), ([15, 16], 0)]\n",
      "TF ragged data from disk\n",
      "[list([1, 14, 22, 16, 43, 530, 32])\n",
      " list([1, 194, 1153, 194, 8255, 78, 95])\n",
      " list([1, 14, 47, 8, 30, 31, 7, 4, 249, 7, 129, 13])]\n",
      "lofas\n",
      "[1, 14, 22, 16, 43, 530, 32]\n",
      "[1, 194, 1153, 194, 8255, 78, 95]\n",
      "[1, 14, 47, 8, 30, 31, 7, 4, 249, 7, 129, 13]\n"
     ]
    }
   ],
   "source": [
    "# Use pickle to write/read imdb_fake data to/from disk\n",
    "# import pickle\n",
    "\n",
    "# Save to disk\n",
    "with open(\"./imdb_fake_data.pkl\", \"wb\") as f:\n",
    "   pickle.dump(imdb_fake_data, f)\n",
    "\n",
    "# Load from disk\n",
    "with open(\"./imdb_fake_data.pkl\", \"rb\") as f:\n",
    "   imdb_fake_ragged_data = pickle.load(f)\n",
    "\n",
    "print(\"Orig fake data\")\n",
    "print(imdb_fake_data)\n",
    "print(\"fake data from disk\")\n",
    "print(imdb_fake_ragged_data)   \n",
    "\n",
    "\n",
    "#-----------------------------------------------------------------\n",
    "with open(\"./ragged_array.pkl\", \"rb\") as f:\n",
    "   tf_ragged_data = pickle.load(f)\n",
    "\n",
    "print(\"TF ragged data from disk\")\n",
    "print(tf_ragged_data)\n",
    "\n",
    "print(\"lofas\" )\n",
    "for i in tf_ragged_data :\n",
    "   print( i )\n",
    "\n",
    "\n",
    "with open(\"./train_data_as_ragged_array.pkl\", \"rb\") as f:\n",
    "   tf_train_data_as_ragged_array = pickle.load(f)\n",
    "\n",
    "with open(\"./train_labels_as_ragged_array.pkl\", \"rb\") as f:\n",
    "   tf_train_labels_as_ragged_array = pickle.load(f)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewData:  # Custom class to control printing\n",
    "    def __init__(self, data):\n",
    "        self.data = np.array(data, dtype=object)\n",
    "\n",
    "    def __str__(self):  # Override the string representation\n",
    "        return \"[\" + \" \".join(str(tuple(item)) for item in self.data) + \"]\"\n",
    "\n",
    "    def __getitem__(self, index):  # Allow indexing\n",
    "        return self.data[index]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[list([6, 7, 8]) 0]\n"
     ]
    }
   ],
   "source": [
    "data1=ReviewData(imdb_fake_data)\n",
    "print(data1.__getitem__(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i\n",
      "([1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32], 1)\n",
      "i\n",
      "([1, 194, 1153, 194, 8255, 78, 228, 5, 6, 1463, 4369, 5012, 134, 26, 4, 715, 8, 118, 1634, 14, 394, 20, 13, 119, 954, 189, 102, 5, 207, 110, 3103, 21, 14, 69, 188, 8, 30, 23, 7, 4, 249, 126, 93, 4, 114, 9, 2300, 1523, 5, 647, 4, 116, 9, 35, 8163, 4, 229, 9, 340, 1322, 4, 118, 9, 4, 130, 4901, 19, 4, 1002, 5, 89, 29, 952, 46, 37, 4, 455, 9, 45, 43, 38, 1543, 1905, 398, 4, 1649, 26, 6853, 5, 163, 11, 3215, 2, 4, 1153, 9, 194, 775, 7, 8255, 2, 349, 2637, 148, 605, 2, 8003, 15, 123, 125, 68, 2, 6853, 15, 349, 165, 4362, 98, 5, 4, 228, 9, 43, 2, 1157, 15, 299, 120, 5, 120, 174, 11, 220, 175, 136, 50, 9, 4373, 228, 8255, 5, 2, 656, 245, 2350, 5, 4, 9837, 131, 152, 491, 18, 2, 32, 7464, 1212, 14, 9, 6, 371, 78, 22, 625, 64, 1382, 9, 8, 168, 145, 23, 4, 1690, 15, 16, 4, 1355, 5, 28, 6, 52, 154, 462, 33, 89, 78, 285, 16, 145, 95], 0)\n",
      "i\n",
      "([1, 14, 47, 8, 30, 31, 7, 4, 249, 108, 7, 4, 5974, 54, 61, 369, 13, 71, 149, 14, 22, 112, 4, 2401, 311, 12, 16, 3711, 33, 75, 43, 1829, 296, 4, 86, 320, 35, 534, 19, 263, 4821, 1301, 4, 1873, 33, 89, 78, 12, 66, 16, 4, 360, 7, 4, 58, 316, 334, 11, 4, 1716, 43, 645, 662, 8, 257, 85, 1200, 42, 1228, 2578, 83, 68, 3912, 15, 36, 165, 1539, 278, 36, 69, 2, 780, 8, 106, 14, 6905, 1338, 18, 6, 22, 12, 215, 28, 610, 40, 6, 87, 326, 23, 2300, 21, 23, 22, 12, 272, 40, 57, 31, 11, 4, 22, 47, 6, 2307, 51, 9, 170, 23, 595, 116, 595, 1352, 13, 191, 79, 638, 89, 2, 14, 9, 8, 106, 607, 624, 35, 534, 6, 227, 7, 129, 113], 0)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tf_train_data  = tf_train_data_as_ragged_array\n",
    "tf_train_labels = tf_train_labels_as_ragged_array \n",
    "tf_train_labels = tf_train_labels.astype(object)\n",
    "\n",
    "nRows = 25000\n",
    "nCols = 2\n",
    "imdb_pt_data = np.empty(0, dtype=object)\n",
    "imdb_pt_data.resize(nRows)\n",
    "\n",
    "# note: since the revies are long, placed the labels first when I was verifying that the data is being set up correctly\n",
    "for i in range(nRows):\n",
    "  d = tf_train_data[i]\n",
    "  l = tf_train_labels[i]\n",
    "  #print(\"label[\",i, \"] = \", l)\n",
    "  #print(\"data [\",i, \"] = \", d )\n",
    "  ll = (d,l)\n",
    "  imdb_pt_data[i] = ll\n",
    "  #print(\"ll = \", ll )\n",
    "  #print(\"\")\n",
    "\n",
    "\n",
    "for i in range(3):\n",
    "  print(\"i\")\n",
    "  print( imdb_pt_data[i])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Custom Dataset Class\n",
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        review, label = self.data[idx]\n",
    "        return torch.tensor(review), torch.tensor(label)  # Convert to tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Data Loading with Padding\n",
    "def collate_fn(batch):\n",
    "    reviews, labels = zip(*batch)\n",
    "    review_lengths = torch.tensor([len(r) for r in reviews])  # Store original lengths\n",
    "    padded_reviews = pad_sequence(reviews, batch_first=True, padding_value=0) # Pad to max length in batch\n",
    "    labels = torch.stack(labels) # Stack labels\n",
    "    return padded_reviews, labels, review_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I am using data from imdb_pt_data. Note that the array has a few data poits (25) because \n",
    "# I am testing if the data transformation from tf to pt was done OK\n",
    "\n",
    "# Split training and test data. Training gets 80% of the data\n",
    "\n",
    "# \n",
    "\n",
    "# train_data, test_data = train_test_split(imdb_fake_data, test_size=0.2, random_state=42)\n",
    "train_data, test_data = train_test_split(imdb_pt_data, test_size=0.2, random_state=42)\n",
    "\n",
    "train_dataset = IMDBDataset(train_data)\n",
    "test_dataset = IMDBDataset(test_data)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    " #4. Define a Pytorch Dense Model\n",
    "\n",
    "class DenseModel(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):\n",
    "        super(DenseModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)  # Word embeddings\n",
    "        self.fc1 = nn.Linear(embedding_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        # self.f = open(\"./IMDB_outputs.txt\", \"a\")\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        \n",
    "        with open(\"./x1.pkl\", \"wb\") as f:\n",
    "            pickle.dump(x, f)\n",
    "        \n",
    "        embedded = self.embedding(x)  # (batch_size, seq_len, embedding_dim)\n",
    "        \n",
    "        with open(\"./embedded.pkl\", \"wb\") as f:\n",
    "            pickle.dump(embedded, f)\n",
    "        # Create mask *before* embedding. 1 for non-padding, 0 for padding\n",
    "        mask = x != 0  # (batch_size, seq_len) - True for non-padding, False for padding\n",
    "        masked_embedded = embedded * mask.unsqueeze(-1).float() # Apply mask to embeddings\n",
    "\n",
    "        # Average pooling over sequence length (handles variable lengths)\n",
    "        lengths = lengths.unsqueeze(1).float()  # (batch_size, 1)\n",
    "        pooled = masked_embedded.sum(dim=1) / lengths  # Average pool\n",
    "        \n",
    "        \n",
    "        with open(\"./pooled.pkl\", \"wb\") as f:\n",
    "            pickle.dump(pooled, f)\n",
    "        x = self.fc1(pooled)\n",
    "        \n",
    "        \n",
    "        with open(\"./linear1.pkl\", \"wb\") as f:\n",
    "            pickle.dump(x, f)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        \n",
    "        with open(\"./relu.pkl\", \"wb\") as f:\n",
    "            pickle.dump(x, f)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        \n",
    "        with open(\"./linear2.pkl\", \"wb\") as f:\n",
    "            pickle.dump(x, f)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Training and Validation Loops\n",
    "input_dim = 10000  # Adjust size of vocabulary to 10000 \n",
    "embedding_dim = 128  # Size of word embeddings\n",
    "hidden_dim = 256\n",
    "output_dim = 2  # Binary classification (0 or 1)\n",
    "\n",
    "\n",
    "model = DenseModel(input_dim, embedding_dim, hidden_dim, output_dim)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # use gpu if available\n",
    "model.to(device) # move model to gpu\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DenseModel(\n",
      "  (embedding): Embedding(10000, 128)\n",
      "  (fc1): Linear(in_features=128, out_features=256, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (fc2): Linear(in_features=256, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# from torchsummary import summary\n",
    "# summary(model,(3, 224))\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Loss: 0.2731, Accuracy: 85.52%\n"
     ]
    }
   ],
   "source": [
    "# change number of epochs as necessary with the real IMDB data.\n",
    "#  Maybe just 10 will be enough \n",
    "num_epochs = 1\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    model.train()  # Set model to training mode\n",
    "    for padded_reviews, labels, lengths in train_loader:\n",
    "      padded_reviews = padded_reviews.to(device) # move data to gpu\n",
    "      labels = labels.to(device) # move data to gpu\n",
    "      lengths = lengths.to(device) # move data to gpu\n",
    "      optimizer.zero_grad()\n",
    "      outputs = model(padded_reviews, lengths)\n",
    "      loss = criterion(outputs, labels)\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "    # Validation\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():  # Disable gradients during validation\n",
    "        for padded_reviews, labels, lengths in test_loader:\n",
    "          padded_reviews = padded_reviews.to(device) # move data to gpu\n",
    "          labels = labels.to(device) # move data to gpu\n",
    "          lengths = lengths.to(device) # move data to gpu\n",
    "          outputs = model(padded_reviews, lengths)\n",
    "          _, predicted = torch.max(outputs.data, 1) # get the prediction\n",
    "          total += labels.size(0)\n",
    "          correct += (predicted == labels).sum().item() # count correct predictions\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Accuracy: {accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1 tensor([[   1,   65,  517,  ...,    0,    0,    0],\n",
      "        [   1,  526,   34,  ...,    0,    0,    0],\n",
      "        [   1,    5, 1977,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [   1,   14,   22,  ...,    0,    0,    0],\n",
      "        [   1,   14,   20,  ...,    0,    0,    0],\n",
      "        [   1,   13,  566,  ...,    0,    0,    0]])\n",
      "embedded tensor([[[ 1.2617,  0.0112,  1.1848,  ...,  1.4919, -0.4083, -0.6236],\n",
      "         [-1.4636, -0.1120,  0.3118,  ...,  0.0943, -0.5127,  1.7256],\n",
      "         [-1.4218,  0.9444, -0.5894,  ..., -1.1257, -1.1973, -1.6173],\n",
      "         ...,\n",
      "         [ 1.0168, -0.8572, -0.0812,  ..., -0.1236, -1.3896, -1.8434],\n",
      "         [ 1.0168, -0.8572, -0.0812,  ..., -0.1236, -1.3896, -1.8434],\n",
      "         [ 1.0168, -0.8572, -0.0812,  ..., -0.1236, -1.3896, -1.8434]],\n",
      "\n",
      "        [[ 1.2617,  0.0112,  1.1848,  ...,  1.4919, -0.4083, -0.6236],\n",
      "         [ 0.1977, -0.9578, -1.0020,  ...,  0.9786, -0.8330,  0.4770],\n",
      "         [-1.1927,  0.1781, -0.8170,  ..., -0.9214,  1.0342,  0.8680],\n",
      "         ...,\n",
      "         [ 1.0168, -0.8572, -0.0812,  ..., -0.1236, -1.3896, -1.8434],\n",
      "         [ 1.0168, -0.8572, -0.0812,  ..., -0.1236, -1.3896, -1.8434],\n",
      "         [ 1.0168, -0.8572, -0.0812,  ..., -0.1236, -1.3896, -1.8434]],\n",
      "\n",
      "        [[ 1.2617,  0.0112,  1.1848,  ...,  1.4919, -0.4083, -0.6236],\n",
      "         [ 0.0132, -1.1311, -0.3175,  ..., -0.9937, -0.6473, -0.2109],\n",
      "         [ 2.0213, -0.1322,  1.0411,  ..., -0.5743,  1.6610, -0.4508],\n",
      "         ...,\n",
      "         [ 1.0168, -0.8572, -0.0812,  ..., -0.1236, -1.3896, -1.8434],\n",
      "         [ 1.0168, -0.8572, -0.0812,  ..., -0.1236, -1.3896, -1.8434],\n",
      "         [ 1.0168, -0.8572, -0.0812,  ..., -0.1236, -1.3896, -1.8434]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.2617,  0.0112,  1.1848,  ...,  1.4919, -0.4083, -0.6236],\n",
      "         [-1.3581, -0.1601, -0.1149,  ...,  0.5392,  0.1279, -0.1706],\n",
      "         [-0.9949, -0.1617,  1.9644,  ...,  0.7807,  0.1824, -1.6230],\n",
      "         ...,\n",
      "         [ 1.0168, -0.8572, -0.0812,  ..., -0.1236, -1.3896, -1.8434],\n",
      "         [ 1.0168, -0.8572, -0.0812,  ..., -0.1236, -1.3896, -1.8434],\n",
      "         [ 1.0168, -0.8572, -0.0812,  ..., -0.1236, -1.3896, -1.8434]],\n",
      "\n",
      "        [[ 1.2617,  0.0112,  1.1848,  ...,  1.4919, -0.4083, -0.6236],\n",
      "         [-1.3581, -0.1601, -0.1149,  ...,  0.5392,  0.1279, -0.1706],\n",
      "         [-1.0321, -0.1030,  1.6778,  ...,  0.5620, -0.3483, -1.1677],\n",
      "         ...,\n",
      "         [ 1.0168, -0.8572, -0.0812,  ..., -0.1236, -1.3896, -1.8434],\n",
      "         [ 1.0168, -0.8572, -0.0812,  ..., -0.1236, -1.3896, -1.8434],\n",
      "         [ 1.0168, -0.8572, -0.0812,  ..., -0.1236, -1.3896, -1.8434]],\n",
      "\n",
      "        [[ 1.2617,  0.0112,  1.1848,  ...,  1.4919, -0.4083, -0.6236],\n",
      "         [ 0.6925,  1.4349, -1.1917,  ...,  0.7573,  1.7467,  2.2126],\n",
      "         [-0.8300, -0.7844, -1.3983,  ...,  0.9469,  0.5006,  1.5627],\n",
      "         ...,\n",
      "         [ 1.0168, -0.8572, -0.0812,  ..., -0.1236, -1.3896, -1.8434],\n",
      "         [ 1.0168, -0.8572, -0.0812,  ..., -0.1236, -1.3896, -1.8434],\n",
      "         [ 1.0168, -0.8572, -0.0812,  ..., -0.1236, -1.3896, -1.8434]]])\n",
      "pooled tensor([[-0.0859, -0.2705, -0.1927,  ...,  0.0571,  0.1008,  0.0078],\n",
      "        [ 0.0287,  0.0887,  0.0454,  ..., -0.0449,  0.0315,  0.1318],\n",
      "        [ 0.0804, -0.0342, -0.2371,  ...,  0.0235, -0.0046, -0.0913],\n",
      "        ...,\n",
      "        [ 0.0716,  0.0160, -0.1085,  ..., -0.0046, -0.1072, -0.0628],\n",
      "        [ 0.0215, -0.0264, -0.0685,  ...,  0.1568,  0.0938, -0.0035],\n",
      "        [-0.0551,  0.0881, -0.2952,  ...,  0.1788,  0.1178,  0.2149]])\n"
     ]
    }
   ],
   "source": [
    "# Load from disk\n",
    "with open(\"./x1.pkl\", \"rb\") as f:\n",
    "   x1 = pickle.load(f)\n",
    "print(\"x1\",x1)\n",
    "\n",
    "with open(\"./embedded.pkl\", \"rb\") as f:\n",
    "   embedded = pickle.load(f)\n",
    "print(\"embedded\",embedded)\n",
    "\n",
    "with open(\"./pooled.pkl\", \"rb\") as f:\n",
    "   pooled = pickle.load(f)\n",
    "print(\"pooled\",pooled)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relu tensor([[0.1432, 0.1074, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0339, 0.0008, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0827, 0.0917, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.2167, 0.0887, 0.0000,  ..., 0.0000, 0.0000, 0.2044],\n",
      "        [0.0180, 0.0154, 0.0000,  ..., 0.0000, 0.0000, 0.1018]])\n",
      "linear2 tensor([[-0.7950,  0.7901],\n",
      "        [-2.6965,  2.8278],\n",
      "        [-1.0879,  1.1277],\n",
      "        [-0.1420,  0.1573],\n",
      "        [ 0.2944, -0.2822],\n",
      "        [ 0.4024, -0.3651],\n",
      "        [ 0.7997, -0.8183],\n",
      "        [-1.1663,  1.2573]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with open(\"./linear1.pkl\", \"rb\") as f:\n",
    "   linear1 = pickle.load(f)\n",
    "print(\"linear1\",linear1)\n",
    "\n",
    "with open(\"./relu.pkl\", \"rb\") as f:\n",
    "   relu = pickle.load(f)\n",
    "print(\"relu\",relu)\n",
    "\n",
    "with open(\"./linear2.pkl\", \"rb\") as f:\n",
    "   linear2 = pickle.load(f)\n",
    "print(\"linear2\",linear2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Evaluation (After Training) - More Detailed\n",
    "model.eval()  # Set to evaluation mode\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # use gpu if available\n",
    "model.to(device) # move model to gpu\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for padded_reviews, labels, lengths in test_loader:\n",
    "        padded_reviews = padded_reviews.to(device) # move data to gpu\n",
    "        labels = labels.to(device) # move data to gpu\n",
    "        lengths = lengths.to(device) # move data to gpu\n",
    "        outputs = model(padded_reviews, lengths)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        all_predictions.extend(predicted.cpu().numpy())  # Store predictions for later analysis\n",
    "        all_labels.extend(labels.cpu().numpy())      # Store true labels\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "# 7. Additional Evaluation Metrics (using scikit-learn)\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(all_labels, all_predictions))\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(all_labels, all_predictions))\n",
    "\n",
    "\n",
    "# Example of getting predictions for a single review\n",
    "def predict(review_indices):\n",
    "    model.eval()\n",
    "    review_tensor = torch.tensor([review_indices]).to(device)  # Add batch dimension\n",
    "    review_length = torch.tensor([len(review_indices)]).to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model(review_tensor, review_length)\n",
    "        _, predicted = torch.max(output, 1)\n",
    "    return predicted.item()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
