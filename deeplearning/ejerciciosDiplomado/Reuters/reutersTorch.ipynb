{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enumerate is the reason why this function is so short, enumerate is a very \"pythonic\" function\n",
    "def vectorize_sequences(sequences, dimension=10000):\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate (sequences):\n",
    "        results[i, sequence] = 1.\n",
    "    return results\n",
    "\n",
    "def vectorize_labels(labels, dimension=46):\n",
    "    results = np.zeros((len(labels), dimension))\n",
    "    for i, label in enumerate (labels):\n",
    "        results[i, label] = 1.\n",
    "    return results\n",
    "\n",
    "def print_review( item ) :\n",
    "  for i in item :\n",
    "    print (i)\n",
    "  return\n",
    "\n",
    "def decode_to_text() :\n",
    "    word_index = reuters.get_word_index()\n",
    "    reverse_word_index = dict([ (val, key) for (key, val) in word.index.items])\n",
    "    decoded_text = ' '.join( [reverse_word_index.get (i-3, '?') for i in train_data[0]])\n",
    "    print ( decoded_text)\n",
    "    return"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
